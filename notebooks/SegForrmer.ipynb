{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8bfb9c-04d4-461d-a168-363f09d960c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (SegformerForSemanticSegmentation,\n",
    "                          SegformerImageProcessor)\n",
    "\n",
    "from transformers import AutoFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "359532bc-3e3e-4e3a-b53d-9733ba157276",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, image_processor, train=True):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = image_processor\n",
    "        self.train = train\n",
    "\n",
    "        sub_path = \"train\" if self.train else \"valid\"\n",
    "\n",
    "        fnames = []\n",
    "        with open(os.path.join(self.root_dir, sub_path, 'list.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                fnames.append(line.strip())\n",
    "\n",
    "        self.images = sorted(\n",
    "            [os.path.join(self.root_dir, sub_path, 'image', fname)\n",
    "             for fname in fnames if fname.endswith('png')])\n",
    "        self.annotations = sorted(\n",
    "            [os.path.join(self.root_dir, sub_path, 'gt', fname)\n",
    "             for fname in fnames if fname.endswith('png')])\n",
    "\n",
    "        assert len(self.images) == len(self.annotations), \"There must be as \\\n",
    "          many images as there are segmentation maps\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image = torch.Tensor(np.array(Image.open(self.images[idx]).convert('RGB')))\n",
    "        image_tensor = image.permute(2, 0, 1)\n",
    "        segmentation_map = torch.Tensor(np.array(Image.open(self.annotations[idx]).convert('L')))\n",
    "        segmentation_map = segmentation_map == 255\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        # encoded_inputs = self.image_processor(image, segmentation_map,\n",
    "        #                                       return_tensors=\"pt\")        \n",
    "        # for k, v in encoded_inputs.items():\n",
    "        #     encoded_inputs[k].squeeze_()  # remove batch dimension\n",
    "\n",
    "        \n",
    "\n",
    "        # return encoded_inputs\n",
    "        return {\"pixel_values\": image_tensor, \"labels\": torch.tensor(segmentation_map.clone().detach(), dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7211bf0-6a3a-4167-95b2-3fa612d137d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 7484\n",
      "Number of validation examples: 1664\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([256, 256])\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/43872167/ipykernel_2480395/3542152132.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\"pixel_values\": image_tensor, \"labels\": torch.tensor(segmentation_map.clone().detach(), dtype=torch.long)}\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# Data\n",
    "############################################################################\n",
    "\n",
    "root_dir = '../data/ovaskainen23'\n",
    "image_processor = SegformerImageProcessor(reduce_labels=True)\n",
    "\n",
    "train_dataset = SemanticSegmentationDataset(\n",
    "  root_dir=root_dir, image_processor=image_processor)\n",
    "valid_dataset = SemanticSegmentationDataset(\n",
    "  root_dir=root_dir, image_processor=image_processor, train=False)\n",
    "\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(valid_dataset))\n",
    "\n",
    "\"\"\"Let's verify a random example:\"\"\"\n",
    "\n",
    "encoded_inputs = train_dataset[7453]\n",
    "print(encoded_inputs[\"pixel_values\"].shape)\n",
    "print(encoded_inputs[\"labels\"].shape)\n",
    "\n",
    "encoded_inputs[\"labels\"]\n",
    "print(encoded_inputs[\"labels\"].squeeze().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc75c831-a3ea-47ce-81ef-41f387ca3a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3945c71-3125-4ee8-bfac-4b94dc5e8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"nvidia/mit-b0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41291340-79b2-4648-832c-3bc917b74438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/FAC/FGSE/ISTE/asamsu/detect/venv_pytorch_gpu/lib/python3.9/site-packages/transformers/models/segformer/feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SegformerFeatureExtractor {\n",
       "  \"_valid_processor_keys\": [\n",
       "    \"images\",\n",
       "    \"segmentation_maps\",\n",
       "    \"do_resize\",\n",
       "    \"size\",\n",
       "    \"resample\",\n",
       "    \"do_rescale\",\n",
       "    \"rescale_factor\",\n",
       "    \"do_normalize\",\n",
       "    \"image_mean\",\n",
       "    \"image_std\",\n",
       "    \"do_reduce_labels\",\n",
       "    \"return_tensors\",\n",
       "    \"data_format\",\n",
       "    \"input_data_format\"\n",
       "  ],\n",
       "  \"do_normalize\": true,\n",
       "  \"do_reduce_labels\": false,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"SegformerFeatureExtractor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 256,\n",
       "    \"width\": 256\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint, size=(256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a798b4c-9fdd-4e81-93a2-7420da6fc4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Next, we define corresponding dataloaders.\"\"\"\n",
    "\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "# valid_dataloader = DataLoader(valid_dataset, batch_size=4)\n",
    "\n",
    "# batch = next(iter(train_dataloader))\n",
    "\n",
    "# for k, v in batch.items():\n",
    "#     print(k, v.shape)\n",
    "\n",
    "# batch[\"labels\"].shape\n",
    "\n",
    "# batch['labels'].sum() / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c5f4eef0-22b8-4bcf-861d-8cf6e02b41d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'non_frac': 0, 'frac': 1}\n",
    "id2label = {0: 'non_frac', 1: 'frac'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f09607cd-3b9e-4908-b05c-2e7e116defda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# Model\n",
    "############################################################################\n",
    "\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(model_checkpoint,\n",
    "                                                         num_labels=2,\n",
    "                                                         id2label=id2label,\n",
    "                                                         label2id=label2id,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "47c7444b-1d73-4973-9538-b0e30bb3cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\"\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "79c0a9f0-78c6-40cb-b763-55d82d41680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "84a509b5-3d2c-4a90-b22a-1b2748f94969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 50\n",
    "# lr = 0.00006\n",
    "# batch_size = 8\n",
    "\n",
    "# hub_model_id = \"segformer-b0-finetuned-segments-sidewalk-2\"\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     f\"segFormer-test1\",\n",
    "#     learning_rate=lr,\n",
    "#     num_train_epochs=epochs,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     save_total_limit=3,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=20,\n",
    "#     eval_steps=20,\n",
    "#     logging_steps=1,\n",
    "#     eval_accumulation_steps=5,\n",
    "#     load_best_model_at_end=True,\n",
    "#     push_to_hub=True,\n",
    "#     hub_model_id=hub_model_id,\n",
    "#     hub_strategy=\"end\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a523c3b-502b-4f89-afcd-e0761a1c8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"segFormer-test1\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    save_strategy = \"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=1000,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9b798fc3-b15b-4e88-918a-cc9e3cc02dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "  with torch.no_grad():\n",
    "    logits, labels = eval_pred\n",
    "    logits_tensor = torch.from_numpy(logits)\n",
    "    # scale the logits to the size of the label\n",
    "    logits_tensor = nn.functional.interpolate(\n",
    "        logits_tensor,\n",
    "        size=labels.shape[-2:],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    ).argmax(dim=1)\n",
    "\n",
    "    pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "    # currently using _compute instead of compute\n",
    "    # see this issue for more info: https://github.com/huggingface/evaluate/pull/328#issuecomment-1286866576\n",
    "    metrics = metric._compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=len(id2label),\n",
    "            ignore_index=333,\n",
    "            # reduce_labels=feature_extractor.do_reduce_labels,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "    \n",
    "    # add per category metrics as individual key-value pairs\n",
    "    per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
    "    per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
    "\n",
    "    metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n",
    "    metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n",
    "      \n",
    "    # metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in zip(id2label.keys(), per_category_accuracy)})\n",
    "    # metrics.update({f\"iou_{id2label[i]}\": v for i, v in zip(id2label.keys(), per_category_iou)})\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4577d75c-efc2-4885-9d7e-7c72bbfdc7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b78d6e3e-b31c-4e34-a452-cb9065fc4269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/43872167/ipykernel_2480395/3542152132.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\"pixel_values\": image_tensor, \"labels\": torch.tensor(segmentation_map.clone().detach(), dtype=torch.long)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2401' max='46800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2401/46800 09:28 < 2:55:28, 4.22 it/s, Epoch 2.56/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mean Iou</th>\n",
       "      <th>Mean Accuracy</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "      <th>Accuracy Non Frac</th>\n",
       "      <th>Accuracy Frac</th>\n",
       "      <th>Iou Non Frac</th>\n",
       "      <th>Iou Frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.390899</td>\n",
       "      <td>0.490963</td>\n",
       "      <td>0.535808</td>\n",
       "      <td>0.957319</td>\n",
       "      <td>0.966120</td>\n",
       "      <td>0.105495</td>\n",
       "      <td>0.957273</td>\n",
       "      <td>0.024652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.267100</td>\n",
       "      <td>0.495119</td>\n",
       "      <td>0.500234</td>\n",
       "      <td>0.989590</td>\n",
       "      <td>0.999807</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.989590</td>\n",
       "      <td>0.000648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.199477</td>\n",
       "      <td>0.495061</td>\n",
       "      <td>0.500176</td>\n",
       "      <td>0.989588</td>\n",
       "      <td>0.999806</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.989588</td>\n",
       "      <td>0.000535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.153124</td>\n",
       "      <td>0.494887</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>0.114837</td>\n",
       "      <td>0.494887</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>0.100140</td>\n",
       "      <td>0.494887</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>0.085451</td>\n",
       "      <td>0.494887</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>0.085447</td>\n",
       "      <td>0.494887</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>0.071578</td>\n",
       "      <td>0.494887</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.068070</td>\n",
       "      <td>0.494887</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.060856</td>\n",
       "      <td>0.494887</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.055860</td>\n",
       "      <td>0.494887</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.056925</td>\n",
       "      <td>0.494887</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.055749</td>\n",
       "      <td>0.494887</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>0.051355</td>\n",
       "      <td>0.494887</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>0.051015</td>\n",
       "      <td>0.494887</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>0.048851</td>\n",
       "      <td>0.494953</td>\n",
       "      <td>0.500065</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.989774</td>\n",
       "      <td>0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>0.046304</td>\n",
       "      <td>0.495018</td>\n",
       "      <td>0.500129</td>\n",
       "      <td>0.989775</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.989775</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>0.046839</td>\n",
       "      <td>0.495012</td>\n",
       "      <td>0.500124</td>\n",
       "      <td>0.989775</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.989775</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.048083</td>\n",
       "      <td>0.495039</td>\n",
       "      <td>0.500150</td>\n",
       "      <td>0.989775</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.989775</td>\n",
       "      <td>0.000302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.044004</td>\n",
       "      <td>0.496001</td>\n",
       "      <td>0.501104</td>\n",
       "      <td>0.989784</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.989783</td>\n",
       "      <td>0.002219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.042552</td>\n",
       "      <td>0.495394</td>\n",
       "      <td>0.500502</td>\n",
       "      <td>0.989779</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.989779</td>\n",
       "      <td>0.001009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.044019</td>\n",
       "      <td>0.495809</td>\n",
       "      <td>0.500913</td>\n",
       "      <td>0.989785</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.989785</td>\n",
       "      <td>0.001832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='208' max='208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [208/208 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/43872167/ipykernel_2480395/3542152132.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\"pixel_values\": image_tensor, \"labels\": torch.tensor(segmentation_map.clone().detach(), dtype=torch.long)}\n",
      "/tmp/43872167/ipykernel_2480395/3542152132.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\"pixel_values\": image_tensor, \"labels\": torch.tensor(segmentation_map.clone().detach(), dtype=torch.long)}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/FAC/FGSE/ISTE/asamsu/detect/venv_pytorch_gpu/lib/python3.9/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/FAC/FGSE/ISTE/asamsu/detect/venv_pytorch_gpu/lib/python3.9/site-packages/transformers/trainer.py:2278\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/work/FAC/FGSE/ISTE/asamsu/detect/venv_pytorch_gpu/lib/python3.9/site-packages/transformers/trainer.py:2662\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2660\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2662\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2663\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m/work/FAC/FGSE/ISTE/asamsu/detect/venv_pytorch_gpu/lib/python3.9/site-packages/transformers/trainer.py:3467\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3464\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3466\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3467\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3468\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3470\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3471\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3475\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3477\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/work/FAC/FGSE/ISTE/asamsu/detect/venv_pytorch_gpu/lib/python3.9/site-packages/transformers/trainer.py:3719\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3715\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3716\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[1;32m   3717\u001b[0m         )\n\u001b[1;32m   3718\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3719\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3720\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3721\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[63], line 22\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m     19\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m logits_tensor\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# currently using _compute instead of compute\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# see this issue for more info: https://github.com/huggingface/evaluate/pull/328#issuecomment-1286866576\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mid2label\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m333\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# reduce_labels=feature_extractor.do_reduce_labels,\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduce_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# add per category metrics as individual key-value pairs\u001b[39;00m\n\u001b[1;32m     32\u001b[0m per_category_accuracy \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mper_category_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0/mean_iou.py:304\u001b[0m, in \u001b[0;36mMeanIoU._compute\u001b[0;34m(self, predictions, references, num_labels, ignore_index, nan_to_num, label_map, reduce_labels)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute\u001b[39m(\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    296\u001b[0m     predictions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m     reduce_labels: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    303\u001b[0m ):\n\u001b[0;32m--> 304\u001b[0m     iou_result \u001b[38;5;241m=\u001b[39m \u001b[43mmean_iou\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgt_seg_maps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnan_to_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_to_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduce_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m iou_result\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0/mean_iou.py:251\u001b[0m, in \u001b[0;36mmean_iou\u001b[0;34m(results, gt_seg_maps, num_labels, ignore_index, nan_to_num, label_map, reduce_labels)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean_iou\u001b[39m(\n\u001b[1;32m    211\u001b[0m     results,\n\u001b[1;32m    212\u001b[0m     gt_seg_maps,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m     reduce_labels: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    218\u001b[0m ):\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculate Mean Intersection and Union (mIoU).\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m            Per category IoU.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     total_area_intersect, total_area_union, total_area_pred_label, total_area_label \u001b[38;5;241m=\u001b[39m \u001b[43mtotal_intersect_and_union\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_seg_maps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_labels\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# compute metrics\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0/mean_iou.py:200\u001b[0m, in \u001b[0;36mtotal_intersect_and_union\u001b[0;34m(results, gt_seg_maps, num_labels, ignore_index, label_map, reduce_labels)\u001b[0m\n\u001b[1;32m    198\u001b[0m total_area_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((num_labels,), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result, gt_seg_map \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, gt_seg_maps):\n\u001b[0;32m--> 200\u001b[0m     area_intersect, area_union, area_pred_label, area_label \u001b[38;5;241m=\u001b[39m \u001b[43mintersect_and_union\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_seg_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_labels\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     total_area_intersect \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m area_intersect\n\u001b[1;32m    204\u001b[0m     total_area_union \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m area_union\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0/mean_iou.py:151\u001b[0m, in \u001b[0;36mintersect_and_union\u001b[0;34m(pred_label, label, num_labels, ignore_index, label_map, reduce_labels)\u001b[0m\n\u001b[1;32m    147\u001b[0m label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(label)[mask]\n\u001b[1;32m    149\u001b[0m intersect \u001b[38;5;241m=\u001b[39m pred_label[pred_label \u001b[38;5;241m==\u001b[39m label]\n\u001b[0;32m--> 151\u001b[0m area_intersect \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintersect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    152\u001b[0m area_pred_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhistogram(pred_label, bins\u001b[38;5;241m=\u001b[39mnum_labels, \u001b[38;5;28mrange\u001b[39m\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_labels \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    153\u001b[0m area_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhistogram(label, bins\u001b[38;5;241m=\u001b[39mnum_labels, \u001b[38;5;28mrange\u001b[39m\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_labels \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/work/FAC/FGSE/ISTE/asamsu/detect/venv_pytorch_gpu/lib/python3.9/site-packages/numpy/lib/histograms.py:846\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, density, weights)\u001b[0m\n\u001b[1;32m    842\u001b[0m indices[indices \u001b[38;5;241m==\u001b[39m n_equal_bins] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;66;03m# The index computation is not guaranteed to give exactly\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;66;03m# consistent results within ~1 ULP of the bin edges.\u001b[39;00m\n\u001b[0;32m--> 846\u001b[0m decrement \u001b[38;5;241m=\u001b[39m \u001b[43mtmp_a\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbin_edges\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    847\u001b[0m indices[decrement] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;66;03m# The last bin includes the right edge. The other bins do not.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "50c285ee-a9ab-4694-a51c-dc2ec5b84e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a9e75a8f-8edc-4816-a251-922b7966bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fine-tune the model\n",
    "# metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "# image_processor.do_reduce_labels\n",
    "\n",
    "# # define optimizer\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
    "# # move model to GPU\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(100):  # loop over the dataset multiple times\n",
    "#     print(\"Epoch:\", epoch)\n",
    "#     for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "#         # get the inputs;\n",
    "#         pixel_values = batch[\"pixel_values\"].to(device)\n",
    "#         labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "#         loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # evaluate\n",
    "#         with torch.no_grad():\n",
    "#             upsampled_logits = nn.functional.interpolate(\n",
    "#                 logits, size=labels.shape[-2:], mode=\"bilinear\",\n",
    "#                 align_corners=False)\n",
    "#             predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "#             # note that the metric expects predictions + labels as numpy arrays\n",
    "#             metric.add_batch(predictions=predicted.detach().cpu().numpy(),\n",
    "#                              references=labels.detach().cpu().numpy())\n",
    "\n",
    "#         # let's print loss and metrics every 50 batches\n",
    "#         if idx % 50 == 0:\n",
    "#             # currently using _compute instead of compute\n",
    "#             metrics = metric._compute(\n",
    "#                   predictions=predicted.cpu(),\n",
    "#                   references=labels.cpu(),\n",
    "#                   num_labels=len(id2label),\n",
    "#                   ignore_index=-99,\n",
    "#                   reduce_labels=False,\n",
    "#               )\n",
    "\n",
    "#             print(f\"Loss: {loss.item()}  \"\n",
    "#                   f\"Mean_iou: {metrics['mean_iou']}  \"\n",
    "#                   f\"Mean accuracy: {metrics['mean_accuracy']}  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "33382b51-cb74-4220-b0bd-be141624f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5f0427ef-4b41-4c99-b005-d126a4102dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 512])\n",
      "torch.Size([1, 2, 128, 128])\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in this image: ['tumor', 'non_tumor']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################################################\n",
    "# Inference\n",
    "############################################################################\n",
    "\n",
    "image = Image.open('../data/ovaskainen23/test/image/OG1_41_60.png').convert('RGB')\n",
    "image\n",
    "\n",
    "# prepare the image for the model\n",
    "pixel_values = image_processor(\n",
    "    image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "print(pixel_values.shape)\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values=pixel_values)\n",
    "\n",
    "# logits are of shape (batch_size, num_labels, height/4, width/4)\n",
    "logits = outputs.logits.cpu()\n",
    "print(logits.shape)\n",
    "\n",
    "\n",
    "def ade_palette():\n",
    "    \"\"\"ADE20K palette that maps each class to RGB values.\"\"\"\n",
    "    return [[0, 0, 0], [255, 255, 255]]\n",
    "\n",
    "\n",
    "pred_segmentation_map = image_processor.post_process_semantic_segmentation(\n",
    "  outputs, target_sizes=[image.size[::-1]])[0]\n",
    "pred_segmentation_map = pred_segmentation_map.cpu().numpy()\n",
    "print(pred_segmentation_map)\n",
    "\n",
    "color_seg = np.zeros((pred_segmentation_map.shape[0],\n",
    "                      pred_segmentation_map.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "palette = np.array(ade_palette())\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[pred_segmentation_map == label, :] = color\n",
    "# Convert to BGR\n",
    "color_seg = color_seg[..., ::-1]\n",
    "\n",
    "# Show image + mask\n",
    "img = color_seg\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Compare this to the ground truth segmentation map:\"\"\"\n",
    "\n",
    "map = Image.open('../data/ovaskainen23/test/gt/OG1_41_60.png').convert('L')\n",
    "map\n",
    "\n",
    "# convert map to NumPy array\n",
    "map = np.array(map)\n",
    "# map = np.array(np.array(map) < 0.5)\n",
    "# map = Image.fromarray(np.uint8(map))\n",
    "# map[map == 0] = 255 # background class is replaced by ignore_index\n",
    "# map = map - 1 # other classes are reduced by one\n",
    "map[map == 255] = 1\n",
    "\n",
    "classes_map = np.unique(map).tolist()\n",
    "unique_classes = ['tumor', 'non_tumor']\n",
    "print(\"Classes in this image:\", unique_classes)\n",
    "\n",
    "# create coloured map\n",
    "color_seg = np.zeros((map.shape[0], map.shape[1], 3), dtype=np.uint8)\n",
    "palette = np.array(ade_palette())\n",
    "\n",
    "# for label, color in enumerate(palette):\n",
    "    # color_seg[map == label, :] = color\n",
    "    \n",
    "# Convert to BGR\n",
    "# color_seg = color_seg[..., ::-1]\n",
    "\n",
    "# Show image + mask\n",
    "# img = color_seg\n",
    "img = map\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "classes_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f9b9351b-2a99-4888-8d1d-66f2929f4198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256, 256), (256, 256))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_segmentation_map.shape, map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f5aa7486-5fc9-47ce-ba8a-0ca9d17508d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_iou 0.483001708984375\n",
      "mean_accuracy 0.5\n",
      "overall_accuracy 0.96600341796875\n",
      "---------------------\n",
      "per-category metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IoU</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>non_frac</th>\n",
       "      <td>0.966003</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frac</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               IoU  accuracy\n",
       "non_frac  0.966003       1.0\n",
       "frac      0.000000       0.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################################################\n",
    "# Metrics\n",
    "############################################################################\n",
    "\n",
    "# metric expects a list of numpy arrays for both predictions and references\n",
    "metrics = metric._compute(\n",
    "                  predictions=[pred_segmentation_map],\n",
    "                  references=[map],\n",
    "                  num_labels=len(id2label),\n",
    "                  reduce_labels=False,\n",
    "                  ignore_index=333,\n",
    "              )\n",
    "\n",
    "metrics.keys()\n",
    "\n",
    "# print overall metrics\n",
    "for key in list(metrics.keys())[:3]:\n",
    "    print(key, metrics[key])\n",
    "\n",
    "# pretty-print per category metrics as Pandas DataFrame\n",
    "metric_table = dict()\n",
    "for id, label in id2label.items():\n",
    "    if id == 255: id = 1\n",
    "    metric_table[label] = [\n",
    "                           metrics[\"per_category_iou\"][id],\n",
    "                           metrics[\"per_category_accuracy\"][id]\n",
    "    ]\n",
    "\n",
    "print(\"---------------------\")\n",
    "print(\"per-category metrics:\")\n",
    "pd.DataFrame.from_dict(metric_table, orient=\"index\",\n",
    "                       columns=[\"IoU\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd7de48-d50a-4185-a663-0316faa88284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
