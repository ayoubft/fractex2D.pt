{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cde4123-2914-420f-8528-02b8f45d4687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from patchify import patchify, unpatchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fab97a2-f093-4aeb-ba95-e1896e475fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17cf3424-8307-446d-aa37-5f0511b85d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.random.rand(700, 700, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "097ad1e6-14e2-4cd1-b81f-269b768a72be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 700, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22aed00f-5b3c-4964-ac44-65d902ea775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = patchify(img, (256, 256, 3), step=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b73d755a-0e88-4ccd-bf84-c59a08c5bf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 256, 256, 3)\n",
      "(4, 1, 256, 256, 3)\n",
      "(4, 1, 256, 256, 3)\n",
      "(4, 1, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "for p in patches:\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "378296f7-3579-4461-a4bb-d58cdbbd8b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4, 1, 256, 256, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed447fc0-fb32-458b-8920-7940e0486d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 3, 1, 1)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(3, 6, 3, 1, 1)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv_trans1 = nn.ConvTranspose2d(6, 3, 4, 2, 1)\n",
    "        self.conv_trans2 = nn.ConvTranspose2d(3, 1, 4, 2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool1(self.conv1(x)))\n",
    "        x = F.relu(self.pool2(self.conv2(x)))        \n",
    "        x = F.relu(self.conv_trans1(x))\n",
    "        x = self.conv_trans2(x)\n",
    "        return x\n",
    "\n",
    "dataset = datasets.MNIST(\n",
    "    root='PATH',\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=2,\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "model = MyModel()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {}, Batch idx {}, loss {}'.format(\n",
    "            epoch, batch_idx, loss.item()))\n",
    "\n",
    "\n",
    "def normalize_output(img):\n",
    "    img = img - img.min()\n",
    "    img = img / img.max()\n",
    "    return img\n",
    "\n",
    "# Plot some images\n",
    "idx = torch.randint(0, output.size(0), ())\n",
    "pred = normalize_output(output[idx, 0])\n",
    "img = data[idx, 0]\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].imshow(img.detach().numpy())\n",
    "axarr[1].imshow(pred.detach().numpy())\n",
    "\n",
    "# Visualize feature maps\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.conv1.register_forward_hook(get_activation('conv1'))\n",
    "data, _ = dataset[0]\n",
    "data.unsqueeze_(0)\n",
    "output = model(data)\n",
    "\n",
    "act = activation['conv1'].squeeze()\n",
    "fig, axarr = plt.subplots(act.size(0))\n",
    "for idx in range(act.size(0)):\n",
    "    axarr[idx].imshow(act[idx])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
